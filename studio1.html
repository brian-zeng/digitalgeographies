<!DOCTYPE html>
<html lang="en-US" class="no-js page">
<head>
    <!-- I don't really know what I'm doing here, but it should all work out. !-->
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!--[if lt IE 9]>
    <script src="https://research.adobe.com/wp-content/themes/adoberesearch.com/js/html5.js"></script>
    <![endif]-->
    <script src="https://use.typekit.net/sxd6aqj.js"></script>
    <script>try{Typekit.load({ async: true });}catch(e){}</script>
    <title>Brian's Research &raquo; This is mine now</title>
    <meta name="description" content="  Image Stylization: History and Future (Part XVII)">
    <meta name="copyright" content="Copyright (c)2016 Adobe Systems Incorporated. All rights reserved." />
    <meta charset="UTF-8">
    <meta content="initial-scale=1, minimum-scale=1, width=device-width" name="viewport">


	<meta name="twitter:card" content="summary_large_image">
	<meta name="twitter:site" content="@AdobeResearch">
	<meta name="twitter:creator" content="@AdobeResearch">
	<meta name="twitter:title" content="Image Stylization: History and Future (Part XVII)">
	<meta name="twitter:description" content="The third and final post in a series on the history of image stylization explores using neural networks to stylize images.">
	<meta name="twitter:image" content="https://research.adobe.com/wp-content/uploads/2018/07/ImageStylization-3-Hero.gif">

	<meta name="twitter:player" content="" />

	<meta property="og:title" content="Image Stylization: History and Future (Part XVII)" />
	<meta property="og:type" content="article" />
	<meta property="og:url" content="https://research.adobe.com/news/image-stylization-history-and-future-part-3" />

	<meta property="og:site_name" content="Adobe Research" />

	<meta property="og:image" content="https://research.adobe.com/wp-content/uploads/2018/07/ImageStylization-3-Hero.gif" />
	<meta property="og:description" content="The third and final post in a series on the history of image stylization explores using neural networks to stylize images." />
	<meta property='og:video' content="" />
    <link rel='dns-prefetch' href='//ajax.googleapis.com' />
<link rel='dns-prefetch' href='//s.w.org' />
<link rel='stylesheet' id='wp-block-library-css'  href='https://research.adobe.com/wp-includes/css/dist/block-library/style.min.css?ver=5.2.2' type='text/css' media='all' />
<link rel='stylesheet' id='style-css'  href='https://research.adobe.com/wp-content/themes/adoberesearch.com/style.css?ver=5.2.2' type='text/css' media='all' />
<script type='text/javascript' src='//ajax.googleapis.com/ajax/libs/jquery/1.12.0/jquery.min.js?ver=5.2.2'></script>
<script type='text/javascript' src='//ajax.googleapis.com/ajax/libs/jqueryui/1.11.3/jquery-ui.min.js?ver=5.2.2'></script>
<script type='text/javascript' src='https://research.adobe.com/wp-content/themes/adoberesearch.com/js/all-min.js?ver=5.2.2'></script>
        <script>
            wpsolr_globalError = [];
            window.onerror = function (msg, url, line, col, error) {
                wpsolr_globalError.push({msg: msg, url: url, line: line, error: error});
            };
        </script>


<script>
if (self.location.hostname == "research.adobe.com") {
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-74885207-1', 'auto');
  ga('send', 'pageview');
}
</script>

<link rel="icon" href="https://research.adobe.com/wp-content/uploads/2016/03/cropped-favicon-32x32.png" sizes="32x32" />
<link rel="icon" href="https://research.adobe.com/wp-content/uploads/2016/03/cropped-favicon-192x192.png" sizes="192x192" />
<link rel="apple-touch-icon-precomposed" href="https://research.adobe.com/wp-content/uploads/2016/03/cropped-favicon-180x180.png" />
<meta name="msapplication-TileImage" content="https://research.adobe.com/wp-content/uploads/2016/03/cropped-favicon-270x270.png" />
</head>

<body class="post-template-default single single-post postid-14421 single-format-standard" data-spy="scroll" data-target=".navbar" data-offset="1">

<div id="top" class="site">

<div class="container white" id="header">
        <div class="flex_content header">
            <div class="content">
                <div class="header_fill"></div>
                <ul class="">

                    <li title="Menu" id="header-menu"></li>
                    <li id="homeNav" title="Home" class=" top-menu-selected"><a href="https://research.adobe.com/">Adobe Research</a></li>
                    <li id="peopleNav" title="People" class=""><a href="https://research.adobe.com/people">People</a></li>
                    <li id="researchNav" title="Research" class=""><a href="https://research.adobe.com/research">Research</a></li>
                    <li id="publicationsNav" title="Publications" class=""><a href="https://research.adobe.com/publications">Publications</a></li>
                    <li id="newsNav" title="News" class="selected"><a href="https://research.adobe.com/news">News</a></li>
                    <li id="careersNav" title="Careers" class=""><a href="https://research.adobe.com/careers">Careers</a></li>
                    <li id="programsNav" title="Programs" class=""><a href="https://research.adobe.com/programs">Programs</a></li>
					<li id="videoNav" title="Videos" class=""><a href="https://research.adobe.com/videos">Videos</a></li>
                    <li id="aboutNav" title="About Adobe Research" class=""><a href="https://research.adobe.com/about-adobe-research">About</a></li>
					<li id="searchNav" title="Search" class="seach-nav "><a href="#" onclick="return onSearchClick();">Search <svg width='15' height='15'><use xlink:href='#grey-search-icon' /></svg></a></li>
                </ul>
									<div class="search-icon-wrapper">
						<form id="searchFormMobile" style="padding: 0px; background-color: transparent; display: flex" action="https://research.adobe.com/" method="get">
							<input type="text" name="s" id="search-mobile" class="search-input-box" value="" onblur="searchFocusOut(event)" placeholder="Search"/>
							<svg width='18' height='18' id="search-button-mobile" class="search-button" onclick="onMobileSearchClick()"><use xlink:href='#grey-search-input-icon' /></svg>
							<svg width='18' height='18' id="cancel-button-mobile" class="search-cancel-button"  onclick="onMobileSearchCancelClick()"><use xlink:href='#grey-search-cancel-input-icon' /></svg>
						</form>
					</div>
					<div id="searchFormDiv" class="header-search-box">
						<form id="searchForm" style="padding: 0px; background-color: transparent; display: flex" action="https://research.adobe.com/" method="get">
							<input type="text" name="s" id="search" class="search-input-box" value="" onblur="searchFocusOut(event)"  placeholder="Search"/>
							<svg width='18' height='18' class="search-button" onclick="onDoSearchClick()"><use xlink:href='#grey-search-input-icon' /></svg>
							<svg width='18' height='18' class="search-cancel-button"><use xlink:href='#grey-search-cancel-input-icon' /></svg>
						</form>
					</div>
				                <a href="http://www.adobe.com"><div class="adobe adobe-logo-tab adobe-logo-mobile"></div></a>
            </div>
        </div>
    </div>
<svg display="none">
  <symbol width="15" height="15" viewBox="0 0 18 15" id="grey-search-icon">
	<rect id="Canvas" fill="#fff" opacity="0" width="18" height="18" /><path fill="#999" d="M16.5865,15.107,12.7,11.2215A6.413,6.413,0,1,0,11.2215,12.7l3.886,3.886a1.05,1.05,0,0,0,1.479-1.479ZM3,7.5A4.5,4.5,0,1,1,7.5,12,4.5,4.5,0,0,1,3,7.5Z" />
  </symbol>

  <symbol width="15" height="15" viewBox="0 0 18 15" width="15" id="white-search-icon">
	<rect id="Canvas" fill="#fff" opacity="0" width="18" height="18" /><path fill="#fff" d="M16.5865,15.107,12.7,11.2215A6.413,6.413,0,1,0,11.2215,12.7l3.886,3.886a1.05,1.05,0,0,0,1.479-1.479ZM3,7.5A4.5,4.5,0,1,1,7.5,12,4.5,4.5,0,0,1,3,7.5Z" />
  </symbol>

  <symbol width="15" height="15" viewBox="0 0 18 15" id="red-search-icon">
	<rect id="Canvas" fill="#fff" opacity="0" width="18" height="18" /><path fill="#f00" d="M16.5865,15.107,12.7,11.2215A6.413,6.413,0,1,0,11.2215,12.7l3.886,3.886a1.05,1.05,0,0,0,1.479-1.479ZM3,7.5A4.5,4.5,0,1,1,7.5,12,4.5,4.5,0,0,1,3,7.5Z" />
  </symbol>

  <symbol width="18" height="20" viewBox="0 2 18 18" id="grey-search-input-icon">
	<rect id="Canvas" fill="#fff" opacity="0" width="18" height="18" /><path fill="#999" d="M16.5865,15.107,12.7,11.2215A6.413,6.413,0,1,0,11.2215,12.7l3.886,3.886a1.05,1.05,0,0,0,1.479-1.479ZM3,7.5A4.5,4.5,0,1,1,7.5,12,4.5,4.5,0,0,1,3,7.5Z" />
  </symbol>

  <symbol width="18" height="18" viewBox="2 2 18 18" id="grey-search-cancel-input-icon">
	<rect id="Canvas" fill="#ff13dc" opacity="0" width="18" height="18" /><path fill="#999" d="M13.2425,3.343,9,7.586,4.7575,3.343a.5.5,0,0,0-.707,0L3.343,4.05a.5.5,0,0,0,0,.707L7.586,9,3.343,13.2425a.5.5,0,0,0,0,.707l.707.7075a.5.5,0,0,0,.707,0L9,10.414l4.2425,4.243a.5.5,0,0,0,.707,0l.7075-.707a.5.5,0,0,0,0-.707L10.414,9l4.243-4.2425a.5.5,0,0,0,0-.707L13.95,3.343a.5.5,0,0,0-.70711-.00039Z" />
  </symbol>
</svg>

<script>
function onMobileSearchClick()
{
	document.getElementById("search-mobile").style.display = "block";
	document.getElementById("cancel-button-mobile").style.display = "block";
	document.getElementById("search-button-mobile").src = "/wp-content/uploads/2020/02/search1.png";
	document.getElementById("search-mobile").focus();
}
function onMobileSearchCancelClick()
{
	document.getElementById("search-mobile").style.display = "none";
	document.getElementById("cancel-button-mobile").style.display = "none";
	document.getElementById("search-button-mobile").src = "<svg width='15' height='15'><use xlink:href='#grey-search-icon' /></svg>";
}
var searchClicked = false;
function onSearchClick()
{
	searchClicked = false;

	document.getElementById("searchFormDiv").style.display = "block";
	document.getElementById("search").focus();

	addClassToElement("peopleNav", "hidden");
	addClassToElement("researchNav", "hidden");
	addClassToElement("publicationsNav", "hidden");
	addClassToElement("newsNav", "hidden");
	addClassToElement("careersNav", "hidden");
	addClassToElement("programsNav", "hidden");
	addClassToElement("aboutNav", "hidden");
	addClassToElement("searchNav", "hidden");
}
function searchFocusOut(event)
{
	setTimeout(function(){ if(!searchClicked)
							document.getElementById("searchFormDiv").style.display = "none";
							removeClassFromElement("peopleNav", "hidden");
							removeClassFromElement("researchNav", "hidden");
							removeClassFromElement("publicationsNav", "hidden");
							removeClassFromElement("newsNav", "hidden");
							removeClassFromElement("careersNav", "hidden");
							removeClassFromElement("programsNav", "hidden");
							removeClassFromElement("aboutNav", "hidden");
							removeClassFromElement("searchNav", "hidden");
						}, 100);

}
function onDoSearchClick()
{
	searchClicked = true;
	document.getElementById("searchFormDiv").style.display = "block";
	document.getElementById("searchForm").submit();
}
function addClassToElement(elementId, className) {
  var element, name, arr;
  element = document.getElementById(elementId);
  arr = element.className.split(" ");
  if (arr.indexOf(className) == -1) {
    element.className += (arr.length>0?" ":"") + className;
  }
}
function removeClassFromElement(elementId, className) {
  var element, name, arr;
  element = document.getElementById(elementId);
  arr = element.className.split(" ");
  var classList = "";
  for(var i=0; i<arr.length; arr++) {
	  if (className == arr[i]) {
		continue;
	  }
	  classList += " "+arr[i];
  }
  element.className = classList.trim();
}
</script>

<div id="content" class="site-content content">

<div id="share-nav" class="share-bar">
	<a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fresearch.adobe.com%2Fnews%2Fimage-stylization-history-and-future-part-3%2F" target="_blank">
		<svg class="share-icon" style="padding-bottom:4px;" width="40px" id="Layer_1" version="1.1" viewBox="0 0 512 512" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
			<path d="M277.41,400.519V272.058h42.82l10.705-53.525H277.41v-21.41  c0-21.41,10.726-32.115,32.115-32.115h21.41v-53.525c-10.705,0-23.98,0-42.82,0c-39.341,0-64.23,30.841-64.23,74.935v32.115h-42.82  v53.525h42.82v128.46L277.41,400.519L277.41,400.519z" id="f_9_"/>
		</svg>
		<!--<img src="https://research.adobe.com/wp-content/themes/adoberesearch.com/images/f_logo_RGB-Blue_100.png"></img>-->
	</a>
		<a href="https://twitter.com/share?url=https%3A%2F%2Fresearch.adobe.com%2Fnews%2Fimage-stylization-history-and-future-part-3" target="_blank">
		<svg class="share-icon" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="36px"
			 viewBox="0 0 375 330" xml:space="preserve">
				<path class="st1" d="M153.6,301.6c94.3,0,145.9-78.2,145.9-145.9c0-2.2,0-4.4-0.1-6.6c10-7.2,18.7-16.3,25.6-26.6
					c-9.2,4.1-19.1,6.8-29.5,8.1c10.6-6.3,18.7-16.4,22.6-28.4c-9.9,5.9-20.9,10.1-32.6,12.4c-9.4-10-22.7-16.2-37.4-16.2
					c-28.3,0-51.3,23-51.3,51.3c0,4,0.5,7.9,1.3,11.7c-42.6-2.1-80.4-22.6-105.7-53.6c-4.4,7.6-6.9,16.4-6.9,25.8
					c0,17.8,9.1,33.5,22.8,42.7c-8.4-0.3-16.3-2.6-23.2-6.4c0,0.2,0,0.4,0,0.7c0,24.8,17.7,45.6,41.1,50.3c-4.3,1.2-8.8,1.8-13.5,1.8
					c-3.3,0-6.5-0.3-9.6-0.9c6.5,20.4,25.5,35.2,47.9,35.6c-17.6,13.8-39.7,22-63.7,22c-4.1,0-8.2-0.2-12.2-0.7
					C97.7,293.1,124.7,301.6,153.6,301.6"/>
		</svg>
		<!--<img src="https://research.adobe.com/wp-content/themes/adoberesearch.com/images/Twitter_Social_Icon_Circle_Color.png"></img>-->
	</a>
	<a href="http://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fresearch.adobe.com%2Fnews%2Fimage-stylization-history-and-future-part-3%2F&title=Image+Stylization%3A+History+and+Future+%28Part+3%29&source=https%3A%2F%2Fresearch.adobe.com%2Fnews" target="_blank">
		<svg class="share-icon" style="margin-left:7px; margin-top: 4px;" width="26px" style="enable-background:new 0 0 512 512;" version="1.1" viewBox="0 0 512 512" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
			<path d="M51.326,185.85h90.011v270.872H51.326V185.85z M96.934,55.278C66.127,55.278,46,75.503,46,102.049  c0,26,19.538,46.813,49.756,46.813h0.574c31.396,0,50.948-20.814,50.948-46.813C146.689,75.503,127.727,55.278,96.934,55.278z   M362.339,179.487c-47.779,0-69.184,26.28-81.125,44.71V185.85h-90.038c1.192,25.411,0,270.872,0,270.872h90.038V305.448  c0-8.102,0.589-16.174,2.958-21.978c6.519-16.174,21.333-32.923,46.182-32.923c32.602,0,45.622,24.851,45.622,61.248v144.926H466  V301.398C466,218.199,421.598,179.487,362.339,179.487z"/>
		</svg>
		<!--<img src="https://research.adobe.com/wp-content/themes/adoberesearch.com/images/LI-In-Bug.png"></img>-->
	</a>
</div>
<style>
		.share-bar {
			position: fixed;
			left: 0px;
			top: calc(calc(100vh / 2) - 100px);
			width: 55px;
			height: fit-content;
			background: white !important;
			padding: 8px 5px 5px 8px;
			border-radius: 0 5px 5px 0 !important;
			box-shadow: 2px 4px 6px 0px rgba(0,0,0,0.15);
		}
		.share-icon {
			padding-bottom: 10px;
			fill: #707070;
		}
		.share-icon:hover {
			padding-bottom: 10px;
			fill: #404040;
		}
		div.share-bar > a:nth-child(1) > img {
			width: 4.6rem;
			padding-top: 1rem;
		}
		div.share-bar > a:nth-child(2) > img {
			width: 4.6rem;
			padding-top: 1rem;
			padding-bottom: 1rem;
		}
		div.share-bar > a:nth-child(3) > img{
			width: 4.6rem;
			padding-bottom: 1rem;
		}
	@media only screen and (max-width: 767px) {
		.share-bar {
			position: fixed;
			display: flex;
			justify-content: center;
			top: unset;
			bottom: 0rem;
			gap: 2rem;
			width: 100%;
			height: fit-content;
			background: white !important;
			padding: 8px 5px 0px 5px;
			border-radius: unset;
			box-shadow: unset;
			transition: bottom 0.3s;
			z-index: 99999
		}
		div.share-bar > a:nth-child(1) > img {
			width: 4.6rem;
			padding-top: 1rem;
		}
		div.share-bar > a:nth-child(2) > img {
			width: 4.6rem;
			padding-top: 1rem;
			padding-bottom: 1rem;
		}
		div.share-bar > a:nth-child(3) > img{
			width: 4.6rem;
			padding-bottom: 1rem;
			margin-bottom: -5.4rem;
		}
	}
	@media only screen and (max-device-width: 1024px) and (min-device-width: 768px), (max-width: 1024px) and (min-width: 768px) {
		.share-bar {
			position: fixed;
			display: flex;
			justify-content: center;
			top: unset;
			bottom: 0rem;
			gap: 2rem;
			width: 100%;
			height: fit-content;
			background: white !important;
			padding: 8px 5px 0px 5px;
			border-radius: unset;
			box-shadow: unset;
			transition: bottom 0.3s;
			z-index: 99999
		}
		div.share-bar > a:nth-child(1) > img {
			width: 4.6rem;
			padding-top: 1rem;
		}
		div.share-bar > a:nth-child(2) > img {
			width: 4.6rem;
			padding-top: 1rem;
			padding-bottom: 1rem;
		}
		div.share-bar > a:nth-child(3) > img{
			width: 4.6rem;
			padding-bottom: 1rem;
			margin-bottom: -5.4rem;
		}
	}
	</style>
	<script>
		window.onscroll = function() {
		  var currentScrollPos = window.pageYOffset;
		  var limit = Math.max( document.body.scrollHeight, document.body.offsetHeight, document.documentElement.clientHeight, document.documentElement.scrollHeight, document.documentElement.offsetHeight) - window.innerHeight - 50;
		  if (limit <= currentScrollPos) {
			document.getElementById("share-nav").style.bottom = "-100px";
		  } else {
			document.getElementById("share-nav").style.bottom = "0px";
		  }
		}
	</script>	<div class="container white wrap ">
        <div class="flex_content wrap max_column" >
            <div class="column8 align-left">
                <div class="content">
                    <div class="page_title_custom">
                        <h1>Image Stylization: History and Future (Part XVII)</h1>
						<div class="column8">
							<h3 style="float:left;">July 19, 2018</h3>
						</div>
                    </div>
				</div>
			</div>
		</div>
	</div>
	<div class="container white wrap ">
<!-- Dayworks: removed pad_top style from the div below -->
        <div class="flex_content wrap max_column pad_bottom" >
		<div id="story" class="column8 align-left story">
                <div class="content">

                   <figure class="wp-block-image"><img src="https://research.adobe.com/wp-content/uploads/2018/07/ImageStylization-3-Hero.gif" alt=""><figcaption></figcaption></figure>
                    <h2>Part <b>3: Neural Stylization</b></h2>
<h3><span style="font-weight: 400;">by <a href="https://research.adobe.com/person/aaron-hertzmann/">Aaron Hertzmann</a>, </span><span style="font-weight: 400;">Adobe Research</span></h3>
<p>This is the third and final blog post in a series of three posts on the history of image stylization algorithms. The <a href="https://research.adobe.com/image-stylization-history-and-future/">first post</a> described early procedural methods for stylization, and the <a href="https://research.adobe.com/image-stylization-history-and-future-part-2/">second post</a> described the first example-based stylization methods, which were based on patch-based algorithms.</p>
<p>In 2012, Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton at University of Toronto demonstrated <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks">state-of-the-art performance on image recognition using deep neural networks</a>, setting off an enormous surge of creativity and energy around AI using deep learning. Although deep networks had been developed over the past decades by Hinton, LeCun, and others, their sudden success was now only possible, in part, because of the development of the original <a href="https://link.springer.com/article/10.1007/s11263-015-0816-y">ImageNet classification dataset</a> by Jia Deng and Fei-Fei Li at Stanford, with many collaborators and crowdworkers. The new neural networks seemed to be a general-purpose tool for all sorts of vision problems. These networks learned rich “features” describing the statistics of natural images from this large dataset.</p>
<p>Hence, in 2015, Leon Gatys, then a neuroscience graduate student at University of Tübingen, revisited the texture synthesis question, which I described in the previous post. <a href="http://papers.nips.cc/paper/5633-texture-synthesis-using-convolutional-neural-networks">Could the new deep network features explain human texture perception</a>?</p>
<p>With his collaborators Matthias Bethge and Alexander Ecker, Gatys began from the approach of Portilla and Simoncelli that I described in the previous post, but simplified the statistical representation (using only correlation statistics) but applied them to the much-richer deep networks. The synthesis results they got were really impressive:</p>
<p><a href="http://adoberesearch.ctlprojects.com/wp-content/uploads/2018/07/Part-3-1.jpg"><img class="alignnone size-large wp-image-14430" src="http://adoberesearch.ctlprojects.com/wp-content/uploads/2018/07/Part-3-1-1024x405.jpg" alt="" srcset="https://research.adobe.com/wp-content/uploads/2018/07/Part-3-1-1024x405.jpg 1024w, https://research.adobe.com/wp-content/uploads/2018/07/Part-3-1-768x304.jpg 768w, https://research.adobe.com/wp-content/uploads/2018/07/Part-3-1.jpg 1028w" sizes="(max-width: 1024px) 100vw, 1024px" /></a></p>
<p>It was hard to imagine the patch-based methods being able to create such complex structures so seamlessly. Simply by using these new representations, they were able to dramatically improve on previous texture synthesis algorithms, capturing various large-scale structures that we might not have even thought of as “texture.” (They later found that random weights in a simpler networks also <a href="https://arxiv.org/abs/1606.00021">worked as well as ImageNet-trained features</a> for texture synthesis, indicating that it is either the neural network architecture —or perhaps the sheer number of correlations—and not the training data that is important in this representation.)</p>
<p><strong>Neural Image Stylization</strong></p>
<p>Having discovered that neural network features could capture texture so well, Gatys and his collaborators asked: could neural networks also work for image stylization?</p>
<p><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Gatys_Image_Style_Transfer_CVPR_2016_paper.html">Their approach was as follows</a>. Starting from a photograph and painting, their goal was to produce an image with the <i>texture</i> of the painting, but the overall “content” of the photograph. This meant a numerical optimization problem to balance these goals. Running this optimization produced a result like this:</p>
<p><a href="http://adoberesearch.ctlprojects.com/wp-content/uploads/2018/07/Part-3-2.jpg"><img class="alignnone size-large wp-image-14428" src="http://adoberesearch.ctlprojects.com/wp-content/uploads/2018/07/Part-3-2-1024x319.jpg" alt="" srcset="https://research.adobe.com/wp-content/uploads/2018/07/Part-3-2-1024x319.jpg 1024w, https://research.adobe.com/wp-content/uploads/2018/07/Part-3-2-768x239.jpg 768w, https://research.adobe.com/wp-content/uploads/2018/07/Part-3-2.jpg 1028w" sizes="(max-width: 1024px) 100vw, 1024px" /></a></p>
<p>This is the “Starry Night” (1889) image that the original Image Analogies algorithm would perform poorly on.</p>
<p>Since their paper was first posted online in August 2015, an enormous amount of energy and excitement followed. Several <a href="https://link.springer.com/chapter/10.1007/978-3-319-46475-6_43">fast neural stylization methods</a> were published, in which a neural network is directly trained to produce the output that Gatys’ slower algorithm would have produced. For example, the <a href="http://papers.nips.cc/paper/6642-universal-style-transfer-via-feature-transforms.pdf">WCT</a> method can efficiently transfer new styles without requiring any training. Mobile apps that apparently implemented versions of this method appeared not long afterward. Versions of this algorithm are implemented in several popular apps like Prisma and Facebook Live Video. Together with Gatys, we developed methods for <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Gatys_Controlling_Perceptual_Factors_CVPR_2017_paper.pdf">preserving image color</a> in his method (using a color transfer method from Image Analogies), as well as controlling spatial and scale variations in style. Moreover, it has inspired renewed interest in image stylization as a research area, as well as contributed to discussions around the role of artificial intelligence in producing art.</p>
<p><a href="http://adoberesearch.ctlprojects.com/wp-content/uploads/2018/07/Part-3-3.jpg"><img class="alignnone size-large wp-image-14431" src="http://adoberesearch.ctlprojects.com/wp-content/uploads/2018/07/Part-3-3-1024x341.jpg" alt="" srcset="https://research.adobe.com/wp-content/uploads/2018/07/Part-3-3-1024x341.jpg 1024w, https://research.adobe.com/wp-content/uploads/2018/07/Part-3-3-768x256.jpg 768w, https://research.adobe.com/wp-content/uploads/2018/07/Part-3-3.jpg 1028w" sizes="(max-width: 1024px) 100vw, 1024px" /></a></p>
<p>This work has led to many other new ideas as well. Other colleagues at Adobe and Cornell have shown how to apply these methods to <a href="https://www.cs.cornell.edu/~fujun/files/style-cvpr17/style-cvpr17.html">photorealistic style transfer</a>:</p>
<p><a href="http://adoberesearch.ctlprojects.com/wp-content/uploads/2018/07/Part-3-4.jpg"><img class="alignnone size-large wp-image-14429" src="http://adoberesearch.ctlprojects.com/wp-content/uploads/2018/07/Part-3-4-1024x278.jpg" alt="" srcset="https://research.adobe.com/wp-content/uploads/2018/07/Part-3-4-1024x278.jpg 1024w, https://research.adobe.com/wp-content/uploads/2018/07/Part-3-4-768x208.jpg 768w, https://research.adobe.com/wp-content/uploads/2018/07/Part-3-4.jpg 1028w" sizes="(max-width: 1024px) 100vw, 1024px" /></a></p>
<p>Numerous new extensions to these ideas are being published regularly.</p>
<p><strong>Image Translation Revisited</strong></p>
<p>A more general problem statement is to learn image transformations from a large collection of paired examples. The <a href="https://phillipi.github.io/pix2pix/">pix2pix algorithm</a>, from Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alyosha Efros at UC Berkeley, takes a collection of before/after pairs as input, and trains a neural network to learn to transform one to the other. They were inspired in part by neural networks’ success at natural language translation tasks. Their method can use much more information than the previous neural stylization and analogies algorithms, because it is trained from larger datasets and thus sees many before-and-after examples. It also makes use of the recent <a href="https://arxiv.org/abs/1406.2661">Generative Adversarial Network</a> (GAN) loss to improve the image quality.</p>
<p><a href="http://adoberesearch.ctlprojects.com/wp-content/uploads/2018/07/Part-3-5.jpg"><img class="alignnone size-large wp-image-14426" src="http://adoberesearch.ctlprojects.com/wp-content/uploads/2018/07/Part-3-5-1024x412.jpg" alt="" srcset="https://research.adobe.com/wp-content/uploads/2018/07/Part-3-5-1024x412.jpg 1024w, https://research.adobe.com/wp-content/uploads/2018/07/Part-3-5-768x309.jpg 768w, https://research.adobe.com/wp-content/uploads/2018/07/Part-3-5.jpg 1028w" sizes="(max-width: 1024px) 100vw, 1024px" /></a></p>
<p>However, pix2pix does make the restriction that you need to supply carefully-paired before/after training examples.</p>
<p>One of the most intriguing recent ideas is the <a href="https://junyanz.github.io/CycleGAN/">CycleGAN</a>, which shows how to train image transformation algorithms from collections of before images and after images <i>without</i> requiring correspondences. That is, you can train to convert images of southern France to Monet paintings, without ever needing a photograph to go with each of the Monet training paintings.</p>
<p><a href="http://adoberesearch.ctlprojects.com/wp-content/uploads/2018/07/Part-3-6.jpg"><img class="alignnone size-large wp-image-14425" src="http://adoberesearch.ctlprojects.com/wp-content/uploads/2018/07/Part-3-6-1024x507.jpg" alt="" srcset="https://research.adobe.com/wp-content/uploads/2018/07/Part-3-6-1024x507.jpg 1024w, https://research.adobe.com/wp-content/uploads/2018/07/Part-3-6-768x380.jpg 768w, https://research.adobe.com/wp-content/uploads/2018/07/Part-3-6.jpg 1028w" sizes="(max-width: 1024px) 100vw, 1024px" /></a></p>
<p>In the short time since this was published, there have been several significant follow-ups to handle the cases of multimodal transformation, when one input image could have several outputs, including <a href="https://richzhang.github.io/ideepcolor/">interactive image colorization</a>, <a href="https://junyanz.github.io/BicycleGAN/">BicycleGAN</a>, and <a href="https://arxiv.org/abs/1804.04732">MUNIT</a>. New improvements and applications are appearing regularly.</p>
<p><strong>Where Are We At?</strong></p>
<p>The recent progress in neural stylization has been exhilarating: so many new results, applications, and insights around artistic image synthesis have appeared in a short amount of time, when not a whole lot had been happening for years beforehand. Through new apps and media attention, the idea of computer-generated paintings is reaching the public consciousness in a new way, and people are starting to ask how these tools transform artwork.</p>
<p>However, despite their initial wow-factor, the recent neural stylization methods are quite limited in some ways. If you’re looking for a cool stylization for your profile pictures or Instagram posts, they’re great. On the other hand, the neural methods give very limited control to a professional user that may wish to direct or control the output, something which is <a href="http://www.dgp.toronto.edu/~donovan/anipaint/">much easier with procedural</a> and patch-based methods.</p>
<p>When inspecting the results from neural stylization, there are many visible artifacts: things that the original artists probably would not have done. For example, scene elements from the example appear in the target (e.g., starry night elements showing up a daytime scene). Scene contours and outlines are often broken up in unnatural ways. The CycleGAN is one potential solution to some of these issues, since it can potentially learn from large, unpaired datasets, but it hasn’t been shown in general cases yet. And, although there have been numerous research papers improving the quality and generality of fast stylization, these methods generally have results that don’t even look as good as Gatys’ slower method.</p>
<p>In general, while all of these method create results that are amazing, they still fall far short of what the original artists would do. It seems hard to imagine that, had Van Gogh visited Tübingen during the day, that he would have used the same colors and elements of his <em>Starry Night.</em> Of course, matching the original artists’ output is an unrealistic bar to set. A more realistic goal is to match the quality of what procedural computer graphics algorithms can achieve for 3D models, and we are not there yet either. As examples, here are two procedural graphics algorithms that achieve many styles that example-based stylization methods currently cannot (e.g., see the results in <a href="https://vimeo.com/45851544">this video</a>).</p>
<p>At present, all existing image stylization algorithms are “just texture.” As we saw at the start of this article, both Image Analogies and Neural Stylization arose from generalizations of texture synthesis algorithms, and they still are about “surface appearance” rather than responding to scene content. In contrast, many of the older procedural methods are able to leverage higher-level concepts like <a href="http://sgrabli.net/research/linedrawing-tog2010-final.pdf">object geometry</a> and <a href="https://mingtianzhao.gitlab.io/research/parse2paint/">object identities</a>.</p>
<p>Finally, although neural stylization has attracted the most attention lately, it’s worth noting that patch-based stylization methods (e.g., Image Analogies) can often give better results than neural stylization. Here is an example from the StyLit paper:</p>
<p><a href="http://adoberesearch.ctlprojects.com/wp-content/uploads/2018/07/Part-3-7.jpg"><img class="alignnone size-large wp-image-14427" src="http://adoberesearch.ctlprojects.com/wp-content/uploads/2018/07/Part-3-7-1024x347.jpg" alt="" srcset="https://research.adobe.com/wp-content/uploads/2018/07/Part-3-7-1024x347.jpg 1024w, https://research.adobe.com/wp-content/uploads/2018/07/Part-3-7-768x260.jpg 768w, https://research.adobe.com/wp-content/uploads/2018/07/Part-3-7.jpg 1028w" sizes="(max-width: 1024px) 100vw, 1024px" /></a></p>
<p>The <a href="https://arxiv.org/abs/1705.01088">Deep Image Analogies</a> method has also shown extremely impressive stylization results, for cases where both images have very similar content. Both methods seem to have advantages: neural stylization seems to generalize better, but patch-based methods  seem to work better when the example and targets are more similar. It is also unknown how to interpret the neural style methods; for example, do they copy patches, interpolate patches, somehow create entirely new patches?</p>
<p>For the time being, all these approaches have something to offer when trying to develop new algorithms. Furthermore, the insights from the old procedural methods have yet to be incorporated into the latest methods. Given how much progress has been made in the past two years, I’m excited to see what comes next!</p>
<p><strong>Acknowledgments.</strong> Thanks to Meredith Kunz for help with this post, and to the following for their comments on drafts: Alex Berg, Phillip Isola, Leon Gatys, Eli Schechtman, Eero Simoncelli, Holger Winnemöller, Jimei Yang, and Jun-Yan Zhu.</p>
                </div>
            </div>
        </div>
    </div>		<div class="container  paddingMed_top paddingMed_bottom wrap">
					<div class="flex_content content_center align_self_end padding_Zero">
							<div class="column8">
									<h1>Related Posts</h1>
								</div>
							</div>
			</div>

		<div class="container white wrap pad_bottom">
<!-- Dayworks: Added paddingMed_left style to div below -->
        <div class="flex_content desktop_nowrap padding_Zero">

<div class="column3 paddingMed_bottom">
<a class="color_dark" href="https://research.adobe.com/news/image-stylization-history-and-future-part-2/" style="width:100%;">
    <div class="image240 image-background " style="background-image: url(https://research.adobe.com/wp-content/uploads/2018/07/ImageStylization-2-Hero-1024x576.gif);">
            </div>
    <div>
        <h2 class="title-single-news-card-featured">Image Stylization: History and Future (Part 2)</h2>
        <h3 class="color_dark sub-text-single-news-card-featured">This post, part two of a three-part series, delves into patch-based methods for image stylization.</h3>
    </div>
</a>
</div>

<div class="column3 paddingMed_bottom">
<a class="color_dark" href="https://research.adobe.com/news/image-stylization-history-and-future/" style="width:100%;">
    <div class="image240 image-background " style="background-image: url(https://research.adobe.com/wp-content/uploads/2018/06/Image-1-Refine-1024x576.gif);">
            </div>
    <div>
        <h2 class="title-single-news-card-featured">Image Stylization: History and Future (Part 1)</h2>
        <h3 class="color_dark sub-text-single-news-card-featured">This three-part series of blog posts tells the story (so far) of image stylization algorithms.</h3>
    </div>
</a>
</div>

<div class="column3 paddingMed_bottom">
<a class="color_dark" href="https://research.adobe.com/news/puppetron-your-face-as-a-stylized-work-of-art/" style="width:100%;">
    <div class="image240 image-background " style="background-image: url(https://research.adobe.com/wp-content/uploads/2018/03/puppetron_blog-1024x397.jpg);">
        <div class="video_preview"></div>    </div>
    <div>
        <h2 class="title-single-news-card-featured">Puppetron: Your Face as a Stylized Work of Art</h2>
        <h3 class="color_dark sub-text-single-news-card-featured">If you’ve ever wondered what you might look like as a painting by a famous artist, Puppetron has the answer.</h3>
    </div>
</a>
</div>
		</div>
</div>


	<div class="container footer" id="footer">
		<div class="flex_content footer-content">
			<div class="footer-left-column">
				<ul>
					<li id="footer-copyright" class="copyright-custom">Copyright &copy; 2021 Adobe. All rights reserved.</li>
				</ul>
			</div>
			<div class="footer-right-column">
				<ul>
					<li id="footer-products"><a href="http://www.adobe.com/products/catalog/software.html">Products</a></li>
					<li id="footer-downloads"><a href="http://www.adobe.com/downloads.html">Downloads</a></li>
					<li id="footer-learn_support"><a href="https://helpx.adobe.com/support.html">Learn &amp; Support</a></li>

					<li id="footer-tos"><a href="http://www.adobe.com/legal/terms.html">Terms of Use</a></li>
					<li id="footer-privacy"><a href="http://www.adobe.com/privacy.html">Privacy</a></li>
					<li id="footer-cookies"><a href="http://www.adobe.com/privacy/cookies.html">Cookies</a></li>
				</ul>
			</div>
		</div>
	</div>

        <!-- wpsolr - ajax auto completion nonce -->
        <input type="hidden" id="wpsolr_autocomplete_nonce"
               value="af4852ba2d">

		<script type='text/javascript' src='https://research.adobe.com/wp-content/plugins/wpsolr-pro/wpsolr/core/js/devbridge/jquery.autocomplete.js?ver=21.8'></script>
<script type='text/javascript' src='https://research.adobe.com/wp-content/plugins/wpsolr-pro/wpsolr/core/bower_components/jsurl/url.min.js?ver=21.8'></script>
<script type='text/javascript'>
/* <![CDATA[ */
var wp_localize_script_autocomplete = {"data":{"ajax_url":"https:\/\/research.adobe.com\/wp-admin\/admin-ajax.php","is_show_url_parameters":true,"is_ajax":true,"SEARCH_PARAMETER_S":"s","SEARCH_PARAMETER_SEARCH":"search","SEARCH_PARAMETER_Q":"wpsolr_q","SEARCH_PARAMETER_FQ":"wpsolr_fq","SEARCH_PARAMETER_SORT":"wpsolr_sort","SEARCH_PARAMETER_PAGE":"wpsolr_page","SORT_CODE_BY_RELEVANCY_DESC":"sort_by_relevancy_desc","css_ajax_container_page_title":".page-title","css_ajax_container_page_sort":".woocommerce-ordering","css_ajax_container_results":".products,.results-by-facets","css_ajax_container_pagination":"nav.woocommerce-pagination,.paginate_div","css_ajax_container_pagination_page":"a.page-numbers,a.paginate","css_ajax_container_results_count":".woocommerce-result-count,.res_info","ajax_delay_ms":"","redirect_search_home":"","suggestions_icon":"https:\/\/research.adobe.com\/wp-content\/plugins\/wpsolr-pro\/wpsolr\/core\/images\/wpsolr-ajax-loader.gif","wpsolr_autocomplete_is_active":true,"wpsolr_autocomplete_selector":[],"wpsolr_autocomplete_action":"wdm_return_solr_rows","wpsolr_autocomplete_nonce_selector":"#wpsolr_autocomplete_nonce","wpsolr_is_search_admin":false}};
/* ]]> */
</script>
<script type='text/javascript' src='https://research.adobe.com/wp-content/plugins/wpsolr-pro/wpsolr/core/js/autocomplete_solr.js?ver=21.8'></script>
<script type='text/javascript' src='https://research.adobe.com/wp-content/plugins/wpsolr-pro/wpsolr/core/js/loadingoverlay/loadingoverlay.min.js?ver=21.8'></script>
<script type='text/javascript' src='https://research.adobe.com/wp-includes/js/wp-embed.min.js?ver=5.2.2'></script>
</body>
</html>
